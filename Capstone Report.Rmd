---
title: 'HarvardX: PH125.9 Data Science - Movielens Project'
author: "George Cedeño"
date: "3/6/2021"
output: 
   pdf_document:
     toc: true
     toc_depth: 3
     number_sections: true
---


# Overview

This project is related to the MovieLens Project of the HervardX: PH125.9x Data Science: Capstone course. The report begins with a general idea and objective of the project.

Then the given dataset will be prepared and setup. An exploratory data analysis is carried out in order to develop a machine learning algorithm that could predict movie ratings until a final model. Results will be explained. Finally the report ends with some concluding remarks.

## Introduction

Recommendation systems use ratings that users have given to items to make specific recommendations. Companies that sell many products to many customers and permit these customers to rate their products, like Amazon, are able to collect massive datasets that can be used to predict what rating a particular user will give to a specific item. Items for which a high rating is predicted for a given user are then recommended to that user. 

For this project, rated movies are analyzed. Recommendation systems are one of the most commonly used models in machine learning algorithms. In fact the success of Netflix is said to be based on its strong recommendation system. The Netflix prize (open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films), in fact, represent the high importance of algorithm for products recommendation system.

For this project I will create a movie recommendation system using the 10M version of MovieLens dataset, collected by GroupLens Research.


## AIM:

The aim of this project is to train a machine learning algorithm that predicts user ratings (from 0.5 to 5 stars) using the inputs of a provided subset (edx dataset) to predict movie ratings in the provided validation set.

The value used to evaluate algorithm performance is the Root Mean Square Error, or RMSE. RMSE is a measure of the differences between values predicted by a model and the values observed. RMSE is a measure of accuracy, to compare forecasting errors of different models for a particular dataset; a lower RMSE is better than a higher one. The effect of each error on RMSE is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSE. Consequently, RMSE is sensitive to outliers.
Five models will be developed and their resulting RMSE will be compared in order to assess their quality. The evaluation criteria for this algorithm is to not use the validation set until the end to assess quality of the model.

The function that computes the RMSE for vectors of ratings and their corresponding predictors will be the following:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$

```{r RMSE_function1, echo = FALSE}
RMSE <- function(predicted_ratings, true_ratings){
  sqrt(mean((predicted_ratings - true_ratings)^2))
}
```

Finally, the best resulting model will be used to predict the movie ratings.

## Dataset

The MovieLens dataset is automatically downloaded

• [MovieLens 10M dataset] https://grouplens.org/datasets/movielens/10m/

• [MovieLens 10M dataset - zip file] http://files.grouplens.org/datasets/movielens/ml-10m.zip


```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#############################################################
# Create edx set, validation set, and submission file
#############################################################
# Note: this process could take a couple of minutes for loading required package: tidyverse and package caret
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
```

In order to predict in the most possible accurate way the movie rating of the users that haven’t seen the movie yet, the he MovieLens dataset will be splitted into 2 subsets that will be the “edx”, a training subset to train the algorithm, and “validation” a subset to test the movie ratings.  

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# The Validation subset will be 10% of the MovieLens data.
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx subset:
validation <- temp %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

# Training set used will be 50% of edx data to train algorithm's
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.5, list = FALSE)
train <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set
test <- temp %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

# Add rows removed from validation set back into train set
removed <- anti_join(temp, test)
train <- rbind(train, removed)
```

As algorithm development is to be carried out on the "edx" subset only (90% of the data), the edx dataset was split again into separate training & test sets for model development. The "validation" subset (10% of the data) will be used in the end to test the final algorithm.
\pagebreak

# Methods and Analysis

## Data Analysis

To get familiar with the dataset, we find the first rows of "train" subset as below.
The subset contain the six variables “userID”, “movieID”, “rating”, “timestamp”, “title”, and “genres”. Each row represent a single rating of a user for a single movie.

```{r head, message = FALSE, warning = FALSE, echo = FALSE}
head(train)
```

A summary of the subset confirms that there are no missing values.

```{r summary, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1)
summary(train)
```

The total of unique users, movies, and genres in the train subset is 69,878 unique users,  10,677 different movies, and 797 genres:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1)
train %>% 
  summarize(n_users = n_distinct(userId), 
            n_movies = n_distinct(movieId), 
            n_genres = n_distinct(genres)) 
```

Users have a preference to rate movies rather higher than lower as shown by the distribution of ratings below. 4 is the most common rating, followed by 3 and 5. 0.5 is the least common rating. In general, half rating are less common than whole star ratings.

```{r rating_distribution, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1)
train %>% 
  ggplot(aes(rating)) + 
  geom_histogram(binwidth = 0.25, color = "black") + 
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) + 
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000))) + 
  ggtitle("Rating distribution") 
```

We can observe that some movies have been rated more often than others, while some have very few ratings and sometimes only one rating. This will be important for our model as very low rating numbers might results in untrustworthy estimate for our predictions. In fact, ~300 movies have been rated only once. 

Thus regularization and a penalty terms will be applied to the models in this project. Regularization is a technique used to reduce the error by fitting a function appropriately on the given training set and avoid overfitting (the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably). Regularization is a technique used for tuning the function by adding an additional penalty term in the error function. The additional term controls the excessively fluctuating function such that the coefficients don’t take extreme values.


```{r number_of_ratings_per_movie, echo = TRUE, message = FALSE, warning = FALSE, fig.height=4, fig.width=5}
train %>%
  count(movieId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = "black") +
  scale_x_log10() +
  xlab("Number of ratings") +
  ylab("Number of movies") +
  ggtitle("Number of ratings per movie") 
```

As 20 movies that were rated only once appear to be obscure, predictions of future ratings for them will be difficult.

```{r obscure_movies, echo = TRUE, message = FALSE, warning = FALSE, fig.height=4, fig.width=5}
train %>%
  group_by(movieId) %>%
  summarize(count = n()) %>%
  filter(count == 1) %>%
  left_join(train, by = "movieId") %>%
  group_by(title) %>%
  summarize(rating = rating, n_rating = count) %>%
  slice(1:20) %>%
  knitr::kable()  
```


We can observe that the majority of users have rated between 30 and 100 movies. So, a user penalty term need to be included later in our models.


```{r number_ratings_given_by_users, echo = TRUE, message = FALSE, warning = FALSE, fig.height=4, fig.width=5}
train %>%
  count(userId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = "black") +
  scale_x_log10() +
  xlab("Number of ratings") + 
  ylab("Number of users") +
  ggtitle("Number of ratings given by users") 
```

Furthermore, users differ vastly in how critical they are with their ratings. Some users tend to give much lower star ratings and some users tend to give higher star ratings than average. The visualization below includes only users that have rated at least 100 movies.

```{r Mean_movie_ratings_given_by_users, echo = TRUE, message = FALSE, warning = FALSE, fig.height=4, fig.width=5}
train %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "black") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
```


## Modelling Approach

We write now the loss-function, previously anticipated, that compute the RMSE, defined as follows:

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


with N being the number of user/movie combinations and the sum occurring over all these combinations.

The RMSE is our measure of model accuracy.

We can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If its result is larger than 1, it means that our typical error is larger than one star, which is not a good result.

The written function to compute the RMSE for vectors of ratings and their corresponding predictions is:

```{r RMSE_function2, echo = TRUE}
set.seed(1)
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The lower the better, as said previously.


### I. Average movie rating model

The first basic model predicts the same rating for all movies, so we compute the dataset’s mean rating. The expected rating of the underlying data set is between 3 and 4.
We start by building the simplest possible recommender system by predicting the same rating for all movies regardless of user who give it. A model based approach assumes the same rating for all movie with all differences explained by random variation :
$$ Y_{u, i} = \mu + \epsilon_{u, i} $$
with $\epsilon_{u,i}$ independent error sample from the same distribution centered at 0 and $\mu$ the “true” rating for all movies. This very simple model makes the assumption that all differences in movie ratings are explained by random variation alone. We know that the estimate that minimizes the RMSE is the least square estimate of $Y_{u,i}$ , in this case, is the average of all ratings:
The expected rating of the underlying data set is between 3 and 4.

```{r, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
mu_hat <- mean(train$rating)
mu_hat
```


If we predict all unknown ratings with $\mu$ or mu_hat, we obtain the first naive RMSE:

```{r naive_rmse, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
naive_rmse <- RMSE(test$rating, mu_hat)
naive_rmse 
```


Here, we represent results table with the first RMSE:

```{r rmse_results1, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
rmse_results <- data.frame(model="Naive Mean-Baseline Model", RMSE=naive_rmse)
rmse_results %>% knitr::kable()
```

This give us our baseline RMSE to compare with next modelling approaches.

In order to do better than simply predicting the average rating, we incorporate some of insights we gained during the exploratory data analysis.


### II.  Movie effect model

To improve above model we focus on the fact that, from experience, we know that some movies are just generally rated higher than others. Higher ratings are mostly linked to popular movies among users and the opposite is true for unpopular movies. We compute the estimated deviation of each movies’ mean rating from the total mean of all movies $\mu$. The resulting variable is called "b" ( as bias ) for each movie "i" $b_{i}$, that represents average ranking for movie $i$:
$$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$

The histogram is left skewed, implying that more movies have negative effects

```{r Number_of_movies_with_the computed_b_i, echo = TRUE, message = FALSE, warning = FALSE, fig.height=3, fig.width=4}
set.seed(1)
movie_avgs <- train %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu_hat))
movie_avgs %>% qplot(b_i, geom="histogram", bins = 10, data = ., color = I("black"), 
                     ylab = "Number of Movies", main = "Number of Movies with computed b_i")
```

This is called the penalty term movie effect.

Our prediction improve once we predict using this model.

```{r predicted_ratings, echo = TRUE, message = FALSE, warning = FALSE}
set.seed (1)
predicted_ratings <- mu_hat + test %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_1_rmse <- RMSE(predicted_ratings, test$rating)
rmse_results <- bind_rows(rmse_results,
                          data.frame(model="Movie effect model",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable() 
```

So we have predicted movie rating based on the fact that movies are rated differently by adding the computed $b_{i}$ to $\mu$. If an individual movie is on average rated worse that the average rating of all movies $\mu$ , we predict that it will rated lower that $\mu$ by $b_{i}$, the difference of the individual movie average from the total average.

We can see an improvement but this model does not consider the individual user rating effect.


### III. Movie and user effect model

We compute the average rating for user $\mu$, for those that have rated over 100 movies, said penalty term user effect. In fact users affect the ratings positively or negatively.
```{r, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
user_avgs <- train %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  filter(n() >= 100) %>% 
  summarize(b_u = mean(rating - mu_hat - b_i))
user_avgs %>% qplot(b_u, geom="histogram", bin=30, data=., color=I("black"), 
                    ylab = "Number of Movies", main = "Number of Movies with computed b_i and b_u")
```

There is substantial variability across users as well: some users are very cranky and other love every movie. This implies that further improvement to our model my be:
$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$
where $b_{u}$ is a user-specific effect. If a cranky user (negative $b_{u}$ rates a great movie (positive $b_{i}$), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.

We compute an approximation by computing $\mu$ and $b_{i}$, and estimating  $b_{u}$, as the average of $$Y_{u, i} - \mu - b_{i}$$

```{r user_avgs, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
user_avgs <- train %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu_hat - b_i))
```

We can now construct predictors and see RMSE improves:

```{r model_2_rmse, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
predicted_ratings <- test %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu_hat + b_i + b_u) %>%
  pull(pred)
model_2_rmse <- RMSE(predicted_ratings, test$rating)
rmse_results <- bind_rows(rmse_results,
                          data.frame(model="Movie and user effect model",  
                                     RMSE = model_2_rmse))
rmse_results %>% knitr::kable()
```

Our rating predictions further reduced the RMSE. But we made stil mistakes on our first model (using only movies). The supposes “best “ and “worst “movie were rated by few users, in most cases just one user. These movies were mostly obscure ones. This is because with a few users, we have more uncertainty. Therefore larger estimates of $b_{i}$, negative or positive, are more likely.
Large errors can increase our RMSE. 


### IV. Movie, user, and Genre effect model

We computed the average popularity for movies $\mu$ and then include the penalty term for genre effect. Genres can also affect ratings positively or negatively based on user preference.
```{r, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
genre_pop <- train %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu_hat - b_i - b_u))
```

There is substantial variability across genres as well: not every user is interested in every genre. This implies that further improvement to our model my be:
$$Y_{u, i, g} = \mu + b_{i} + b_{u} + b_{g} + \epsilon_{u, i, g}$$
where $b_{g}$ is a genre-specific effect. If a sad user (negative $b_{u}$) wants to see happy  movies all month (positive $b_{g}$) but ends up watching a tear-jerker movie that was recommended simply due to its rating (positive $b_{i}$), the effects are conditional (negative $b_{i}$) and we may inaccurately predict that this user gave this great movie a 5 rather than a 3.

We compute an approximation by computing $\mu$, $b_{i}$, $b_{u}$ and estimating $b_{g}$, as the average of $$Y_{u, i} = \mu - b_{i} - b_{u}$$

We can now construct predictors and see RMSE improves:

```{r model_3_rmse, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
predicted_ratings <- test %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_pop, by='genres') %>%
  mutate(pred = mu_hat + b_i + b_u + b_g) %>%
  pull(pred)
model_3_rmse <- RMSE(predicted_ratings, test$rating)
rmse_results <- bind_rows(rmse_results,
                          data.frame(model="Movie, user, and genre effect model",  
                                     RMSE = model_3_rmse))
rmse_results %>% knitr::kable()
```

Our rating predictions further reduced the RMSE. But we made mistakes on our second model (using only movies and user effects). Genres can affect ratings and the moods of users. If a really upset user saw a low-rated movie with a sweet ending, the rating may increase... sparking more interest amongst users that enjoy happy movies. Understanding which genre's users gravitate towards may  reduce uncertainty. Therefore, since a users genre-preference varies, larger estimates of $b_{u}$, negative or positive, are more likely.
Again, large errors can increase our RMSE. 

Until now, we computed standard error and constructed confidence intervals to account for different levels of uncertainty. However, when making predictions, we need one number, one prediction, not an interval. For this we introduce the concept of regularization, that permits to penalize large estimates that come from small sample sizes. The general idea is to add a penalty for large values of $b_{i}$ to the sum of squares equation that we minimize. So having many large $b_{i}$, make it harder to minimize. Regularization is a method used to reduce the effect of overfitting.


### V. Regularized movie and user effect model

So estimates of $b_{i}$, $b_{u}$, and $b_{g}$ are caused by movies with very few ratings, some users that only rated a very small number of movies, and genre-specific preferences. Hence these  can strongly influence the prediction. The use of the regularization permits to penalize these aspects. We should find the value of lambda (that is a tuning parameter) that will minimize the RMSE. This shrinks the $b_{i}$, $b_{u}$, and $b_{g}$ in case of small number of ratings.


```{r lambdas, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
set.seed(1)
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  # Calculate the average of all ratings in 90% of data
  mu <- mean(edx$rating)
  # Add movie effect
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  # Add user effect
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  # Add genre effect
  b_g <- edx %>% 
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by='userId') %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+l))
  # Compute the predicted ratings on validation dataset (10% of data)
  predicted_ratings <- validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(pred = mu + b_i + b_u + b_g) %>%
    pull(pred)
  # Predict the RMSE on the validation set (10% of data)
  return(RMSE(predicted_ratings, validation$rating))
})
```


We plot RMSE vs lambdas to select the optimal lambda

```{r plot_lambdas, echo = TRUE, message = FALSE, warning = FALSE}
qplot(lambdas, rmses)  
```

For the full model, the optimal lambda is:

```{r min_lambda, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
lambda <- lambdas[which.min(rmses)]
lambda
```

For the full model, the optimal lambda is: 5.25

The new results will be:

```{r rmse_results2, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(1)
rmse_results <- bind_rows(rmse_results,
                          data_frame(model="Regularized movie and user effect model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

\pagebreak

# Results

The RMSE values of all the represented models are the following:

```{r rmse_results3, echo = FALSE}
rmse_results %>% knitr::kable()
```

We therefore found the lowest value of RMSE that is 0.8646.


# Discussion

So we can confirm that the final model for our project is the following:

$$Y_{u, i, g} = \mu + b_{i} + b_{u} + b_{g} + \epsilon_{u, i, g}$$

This model works well if the average user doesn't rate a particularly good/popular movie with a large positive $b_{i}$, by disliking a particular movie. 


# Conclusion

We can affirm to have built a machine learning algorithm to predict movie ratings with MovieLens dataset.
The regularized model including the effect of user is characterized by the lower RMSE value and is hence the optimal model to use for the present project.
The optimal model characterised by the lowest RMSE value (0.8646). 
Other machine learning models may improve the results further, but hardware limitations, as the RAM, are a constraint.

\pagebreak

# Appendix - Enviroment

```{r}
print("Operating System:")
version
```











